# -*- coding: utf-8 -*-
"""climate prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-VJQpzTIUWKMmt6kFR4UaTzF8nJBKuJB
"""



"""# Task
Simulate or collect climate data, preprocess it, train a machine learning model for climate prediction, evaluate the model, and save the trained model.

## Data acquisition

### Subtask:
Determine the type of climate data needed (e.g., temperature, humidity, wind speed, precipitation) and how to obtain it. This could involve simulating data based on known climate patterns or finding existing datasets.

## Data preprocessing

### Subtask:
Clean and prepare the data for training. This may include handling missing values, scaling features, and splitting the data into training and testing sets.

**Reasoning**:
The first step is to load the climate data. Since the previous step was data acquisition planning, I will simulate some data for this subtask.
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Simulate climate data
np.random.seed(42)
dates = pd.date_range('2000-01-01', periods=1000, freq='D')
temperature = np.random.uniform(low=-10, high=35, size=1000)
humidity = np.random.uniform(low=30, high=95, size=1000)
wind_speed = np.random.uniform(low=0, high=20, size=1000)
precipitation = np.random.uniform(low=0, high=10, size=1000)

# Introduce some missing values
temperature[100:110] = np.nan
humidity[200:210] = np.nan

df = pd.DataFrame({
    'date': dates,
    'temperature': temperature,
    'humidity': humidity,
    'wind_speed': wind_speed,
    'precipitation': precipitation
})

# Inspect for missing values
print("Missing values before handling:")
print(df.isnull().sum())

# Handle missing values: Impute with mean for numerical columns
for col in ['temperature', 'humidity', 'wind_speed', 'precipitation']:
    if df[col].isnull().any():
        mean_val = df[col].mean()
        df[col].fillna(mean_val, inplace=True)

print("\nMissing values after handling:")
print(df.isnull().sum())

# Identify features for scaling and apply StandardScaler
# Date is not a feature for scaling in this context, assuming we'll use numerical features
features_to_scale = ['temperature', 'humidity', 'wind_speed', 'precipitation']
scaler = StandardScaler()
df[features_to_scale] = scaler.fit_transform(df[features_to_scale])

# Split data into features (X) and target (y) - Assuming temperature prediction for simplicity
# For time series, splitting is more complex, but for this simulation, a simple split is used
X = df[['humidity', 'wind_speed', 'precipitation']]
y = df['temperature']

# Split data into training and testing sets (80/20 ratio)
# For time series data, a simple train_test_split is not ideal.
# A more appropriate method would be time-based splitting.
# However, for this simulation and to demonstrate the general process,
# we will use train_test_split for simplicity as specified in the instructions.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display shapes of training and testing sets
print("\nShapes of training and testing sets:")
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

# Display first few rows of the training set
print("\nFirst few rows of the training set (X_train):")
display(X_train.head())

"""## Model selection

### Subtask:
Choose an appropriate machine learning model for time series prediction or regression, depending on the nature of the climate data and the prediction task.

**Reasoning**:
Based on the previous steps, the data is prepared for a regression task where temperature is the target variable. I need to identify and choose suitable regression models. Common models for regression include Linear Regression, Ridge, Lasso, Elastic Net, Support Vector Regression, Decision Trees, Random Forests, and Gradient Boosting models like Gradient Boosting Regressor, XGBoost, and LightGBM. Given the non-linear nature that might be present in climate data relationships, and the need for robustness, I will choose two models: Linear Regression as a baseline and Random Forest Regressor for its ability to handle non-linear relationships and feature interactions.
"""

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor

# 1. Identify suitable machine learning models for regression.
# Suitable models: Linear Regression, Ridge, Lasso, Elastic Net, Support Vector Regression,
# Decision Tree Regressor, Random Forest Regressor, Gradient Boosting Regressor, XGBoost, LightGBM.

# 2. Select one or two appropriate regression models.
# Selected models: Linear Regression and Random Forest Regressor.

# 3. Briefly explain why the chosen model(s) are appropriate.
print("Chosen Models:")
print("1. Linear Regression:")
print("   - Simple and interpretable baseline model.")
print("   - Assumes a linear relationship between features and target.")
print("   - Good for understanding the basic relationship between climate features and temperature.")
print("\n2. Random Forest Regressor:")
print("   - Ensemble model that can capture non-linear relationships and interactions between features.")
print("   - Less prone to overfitting compared to single decision trees.")
print("   - Generally provides good performance on a variety of regression tasks, including those with potentially complex patterns like climate data.")

"""## Model training

### Subtask:
Train the selected model using the preprocessed training data.

**Reasoning**:
Import the selected models, instantiate them, and train them using the training data.
"""

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor

# Instantiate the chosen models
linear_reg_model = LinearRegression()
rf_reg_model = RandomForestRegressor(random_state=42) # Using random_state for reproducibility

# Train the models using the training data
linear_reg_model.fit(X_train, y_train)
rf_reg_model.fit(X_train, y_train)

print("Linear Regression model trained.")
print("Random Forest Regressor model trained.")

"""## Model evaluation

### Subtask:
Evaluate the trained model's performance using the testing data and appropriate metrics.

**Reasoning**:
Import the necessary metrics for model evaluation and make predictions using the trained models on the test data.
"""

from sklearn.metrics import mean_squared_error, r2_score

# Make predictions using the trained Linear Regression model
linear_reg_predictions = linear_reg_model.predict(X_test)

# Make predictions using the trained Random Forest Regressor model
rf_reg_predictions = rf_reg_model.predict(X_test)

print("Predictions made for both models.")

"""**Reasoning**:
Calculate and print the evaluation metrics (MSE and R-squared) for both models to assess their performance on the test data.


"""

# Calculate MSE and R-squared for the Linear Regression model
linear_reg_mse = mean_squared_error(y_test, linear_reg_predictions)
linear_reg_r2 = r2_score(y_test, linear_reg_predictions)

# Calculate MSE and R-squared for the Random Forest Regressor model
rf_reg_mse = mean_squared_error(y_test, rf_reg_predictions)
rf_reg_r2 = r2_score(y_test, rf_reg_predictions)

# Print the evaluation metrics
print("\n--- Model Evaluation ---")
print(f"Linear Regression Model:")
print(f"  Mean Squared Error (MSE): {linear_reg_mse:.4f}")
print(f"  R-squared (R2) Score: {linear_reg_r2:.4f}")

print(f"\nRandom Forest Regressor Model:")
print(f"  Mean Squared Error (MSE): {rf_reg_mse:.4f}")
print(f"  R-squared (R2) Score: {rf_reg_r2:.4f}")

"""## Model saving

### Subtask:
Save the trained model so it can be used later for making predictions.

**Reasoning**:
Import the joblib library and save the trained models to files.
"""

import joblib

# Save the trained linear_reg_model
joblib.dump(linear_reg_model, 'linear_regression_model.joblib')

# Save the trained rf_reg_model
joblib.dump(rf_reg_model, 'random_forest_regressor_model.joblib')

print("Trained models saved successfully.")

"""## Summary:

### Data Analysis Key Findings

*   The preprocessing step successfully handled missing values in the simulated data by imputing the mean for numerical columns.
*   Numerical features (temperature, humidity, wind\_speed, precipitation) were successfully scaled using `StandardScaler`.
*   The data was split into training (80%) and testing (20%) sets, resulting in 800 training samples and 200 testing samples.
*   Two regression models, Linear Regression and Random Forest Regressor, were chosen and trained for the prediction task.
*   The evaluation metrics on the test set showed that the Linear Regression model achieved a Mean Squared Error (MSE) of 0.9906 and an R-squared (\(R^2\)) score of -0.0106.
*   The Random Forest Regressor model achieved an MSE of 1.0324 and an \(R^2\) score of -0.0532.
*   Both models exhibited low and negative \(R^2\) scores, indicating poor performance on the test data.
*   The trained Linear Regression and Random Forest Regressor models were successfully saved as `linear_regression_model.joblib` and `random_forest_regressor_model.joblib`, respectively.

### Insights or Next Steps

*   The low \(R^2\) scores suggest that the chosen models or the simulated data may not adequately capture the underlying climate patterns. Further investigation into model selection, feature engineering, or using more realistic climate data is needed.
*   For real-world time series climate data, a time-based splitting approach for training and testing sets should be implemented instead of a random split to avoid data leakage and ensure the model is evaluated on future, unseen data.

"""